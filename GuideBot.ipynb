{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Personal Project: Guide Bot\n",
    "\n",
    "Goal of this project: To build \"Guide Bot\" which can be connected to cameras (possibly of cell phones) to interpret the surrounding (camera input) and provide vocal information using text-to-speech conversion. Could be an aid for the blind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the original dataset:\n",
    "\n",
    "Name: Flickr 30k Data\n",
    "One folder of 30k images and a csv file of corresponding captions of the images\n",
    "Acknowledgement -- this dataset is taken from University of Illinois at Urbana-Champaign Department of Computer Science (https://forms.illinois.edu/sec/229675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acknowledgement for Pretrained Model Used\n",
    "1) Inception V3 (from keras.applications)\n",
    "2) Glove: Global Vectors for Word Representation\n",
    "    by Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.\n",
    "    source from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make necessary folders for the data\n",
    "import os\n",
    "\n",
    "data_dir = './data_folder'\n",
    "os.mkdir(data_dir)\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(data_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "original_data_dir = './flickr30k_images/flickr30k_images'\n",
    "count = 0\n",
    "all_imgs = set()\n",
    "train_imgs = set()\n",
    "val_imgs = set()\n",
    "test_imgs = set()\n",
    "for img in os.listdir(original_data_dir):\n",
    "    if (count == 16000):\n",
    "        break\n",
    "    if (count < 12000):\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(train_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        train_imgs.add(img)\n",
    "        count += 1\n",
    "    elif (count < 14000):\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(validation_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        val_imgs.add(img)\n",
    "        count += 1\n",
    "    else :\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(test_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        test_imgs.add(img)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Trainset images : ', 12002)\n",
      "('Validationset images : ', 2002)\n",
      "('Testset images : ', 2001)\n",
      "16000\n",
      "12000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print('Trainset images : ', len(os.listdir(train_dir)))\n",
    "print('Validationset images : ', len(os.listdir(validation_dir)))\n",
    "print('Testset images : ', len(os.listdir(test_dir)))\n",
    "# \".DS_Store\" was also counted in all of the folders (therefore got 12000/2000/2000 images)\n",
    "print(len(all_imgs))\n",
    "print(len(train_imgs))\n",
    "print(len(val_imgs))\n",
    "print(len(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv into txt with only those that matter (that is, 16k imgs & 80k captions out of 30k & 150k)\n",
    "csv_path = './flickr30k_images/results.csv'\n",
    "with open(csv_path, \"r\") as csv_file:\n",
    "    lines = [line.split(\"|\") for line in csv_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert csv into txt with only those that matter (that is, 16k imgs & 80k captions out of 30k & 150k)\n",
    "txt_file = open('./data_folder/all_captions.txt',\"w\")\n",
    "\n",
    "for line in lines:\n",
    "    img_name = line[0].strip()\n",
    "    if img_name in all_imgs:\n",
    "        txt_file.write(img_name + \"#\" + str(line[1]).strip() + \" \" + line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79714\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for line in open('./data_folder/all_captions.txt',\"r\"):\n",
    "    count += 1\n",
    "print(count)\n",
    "# 286 captions are missing, but will ignore them and proceed to next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary \"imgs_and_captions\" where key : name_of_images and value : corresponding_captions\n",
    "all_captions = open('./data_folder/all_captions.txt',\"r\").read()\n",
    "imgs_and_captions = dict()\n",
    "for line in all_captions.split('\\n'):\n",
    "    l = line.split(' ')\n",
    "    img_name = l[0].split('.')[0]\n",
    "    corresponding_caption = ' '.join(l[1:])\n",
    "    if img_name not in imgs_and_captions:\n",
    "        imgs_and_captions[img_name] = list()\n",
    "    imgs_and_captions[img_name].append(corresponding_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(len(imgs_and_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814037463\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions.keys()[12345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' On the left , a hand points to a surprised looking woman with short hair sitting at a table in front of a window,,,,,,,,\\r', ' A woman sits at a table near a window , with a plate and glass in front of her .,,,,,,,,\\r', ' A woman wearing a floral dress sits as a finger is pointed at her .,,,,,,,,,\\r', ' A woman sitting down to have a meal is being pointed at .,,,,,,,,,\\r', ' A woman has a finger pointing in her face .,,,,,,,,,\\r']\n",
      "number of captions: 5\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions['2814037463'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2814037463'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3030015033\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions.keys()[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' This gentleman is standing outside trying to make something he has his tools on this makeshift table and a couple of men are watching him .,,,,,,,,,\\r', ' A man is doing some work outdoors that requires some tools , like a mallet and chisel , while another man looks on smiling .,,,,,,,\\r', ' A man in a red pullover is creating a work of art on a cement block in the city square while onlookers gleefully watch .,,,,,,,,,\\r', ' A man in a red shirt works on a white sculpture .,,,,,,,,,\\r', ' An artist crating some stone art for tourist .,,,,,,,,,\\r']\n",
      "number of captions: 5\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions['2860314714'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2860314714'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each caption in the dictionary \"imgs_and_captions\"\n",
    "import string\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "for key, captions in imgs_and_captions.items():\n",
    "    for i in range(len(captions)):\n",
    "        tokens = captions[i].split() #split into words\n",
    "        tokens = [word.translate(None, string.punctuation) for word in tokens] #remove punctuations\n",
    "        tokens = [word for word in tokens if word.isalpha()] #remove non-alphabetics\n",
    "        tokens = [word.strip() for word in tokens] #remove trailing whitespaces\n",
    "        tokens = [word.lower() for word in tokens] #convert to lower-case \n",
    "#         tokens = [word for word in tokens if word not in set (stopwords.words('english'))] #remove stopwords        \n",
    "        captions[i] = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on the left a hand points to a surprised looking woman with short hair sitting at a table in front of a window', 'a woman sits at a table near a window with a plate and glass in front of her', 'a woman wearing a floral dress sits as a finger is pointed at her', 'a woman sitting down to have a meal is being pointed at', 'a woman has a finger pointing in her face']\n",
      "number of captions: 5\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions['2814037463'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2814037463'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this gentleman is standing outside trying to make something he has his tools on this makeshift table and a couple of men are watching him', 'a man is doing some work outdoors that requires some tools like a mallet and chisel while another man looks on smiling', 'a man in a red pullover is creating a work of art on a cement block in the city square while onlookers gleefully watch', 'a man in a red shirt works on a white sculpture', 'an artist crating some stone art for tourist']\n",
      "number of captions: 5\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions['2860314714'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2860314714'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people walking through a short tunnel with where are you written on the wall', 'a man walks under a bridge and reads grafitti that reads where are you', 'a man with a cigarette walks past graffitti which says where are you', 'a man is walking next to a wall with where are you painted on it', 'a man walking reads a wall asking where are you']\n",
      "number of captions: 5\n"
     ]
    }
   ],
   "source": [
    "print(imgs_and_captions['2514612680'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2514612680'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('./data_folder/cleaned_captions.txt',\"w\")\n",
    "for img, captions in imgs_and_captions.items():\n",
    "    for caption in captions:\n",
    "        txt_file.write(img + \" \" + caption + '\\n')\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "12000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_imgs))\n",
    "print(len(train_imgs))\n",
    "print(len(val_imgs))\n",
    "print(len(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    }
   ],
   "source": [
    "# A list of names of imgs in the trainset (without .jpg)\n",
    "train_img_names = []\n",
    "for img in train_imgs:\n",
    "    train_img_names.append(img.split('.')[0])\n",
    "print(len(train_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create captions for the training set\n",
    "# Dictionary \"training_captions\" where key : name of image (without jpg extension) value : corresponding captions\n",
    "cleaned_captions = open('./data_folder/cleaned_captions.txt',\"r\").read()\n",
    "training_captions = dict()\n",
    "for line in cleaned_captions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    if (len(tokens) < 2):\n",
    "        continue\n",
    "    img, caption = tokens[0], tokens[1:]\n",
    "    if img in train_img_names:\n",
    "        if img not in training_captions:\n",
    "            training_captions[img] = list()\n",
    "        words = 'startseq ' + ' '.join(caption) + ' endseq'\n",
    "        training_captions[img].append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "3178005751\n",
      "['startseq a man and woman stand next two each other in front of a wood fence endseq', 'startseq two people posing for the picture with a few trees in the background endseq', 'startseq a boy and girl both in jeans are standing in front of a fence endseq', 'startseq a girl in gray and a guy in stripes pose against a fence endseq', 'startseq smiling woman with man in striped sweatshir in the park endseq']\n"
     ]
    }
   ],
   "source": [
    "print(len(training_captions))\n",
    "print(training_captions.keys()[36])\n",
    "print(training_captions['3178005751'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12974441\n",
      "['startseq young man with glasses and two small pigtails on front of his hair with bloody face appears to be in emergency room waiting area endseq', 'startseq an injured bloody person sitting with friends in a hospital waiting room endseq', 'startseq a gentleman in a waiting room of a hospital with burns to his face endseq', 'startseq a man who has been beaten up or has a bad rash on his face endseq', 'startseq person with wounds on their face in a waiting room endseq']\n"
     ]
    }
   ],
   "source": [
    "print(training_captions.keys()[6789])\n",
    "print(training_captions['12974441'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create captions for the validation set too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Inception V3 model and remove its last layer\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model_v3_without_output_layer = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(image_path):\n",
    "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Function to encode a given image into a vector of size (2048, )\n",
    "def encode(image):\n",
    "    image = preprocess(image) # preprocess the image\n",
    "    fea_vec = model_v3_without_output_layer.predict(image) # Get the encoding vector for the image\n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated feature engineering\n",
    "# \"not to classify the image but just get fixed-length informative vector for each image\"\n",
    "# Dictionary \"train_imgs_encoded\" where key : img_name value : feature_vector_of_size_(2048, )\n",
    "train_imgs_encoded = dict()\n",
    "for img in os.listdir(train_dir):\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(train_dir, img)\n",
    "    train_imgs_encoded[img_name] = encode(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the bottleneck train images features\n",
    "import pickle\n",
    "path_for_training = os.path.join(train_dir, 'Pickle')\n",
    "os.mkdir(path_for_training)\n",
    "path_for_training = os.path.join(path_for_training, 'train_imgs_encoded.pkl')\n",
    "with open(path_for_training,'wb') as encoded_pickle:\n",
    "    pickle.dump(train_imgs_encoded, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automated feature engineering\n",
    "# \"not to classify the image but just get fixed-length informative vector for each image\"\n",
    "# Dictionary \"validation_imgs_encoded\" where key : img_name value : feature_vector_of_size_(2048, )\n",
    "validation_imgs_encoded = dict()\n",
    "count = 1\n",
    "for img in os.listdir(validation_dir):\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(validation_dir, img)\n",
    "    print(\"\" + str(count) + \" / 2000 encoding...\")\n",
    "    validation_imgs_encoded[img_name] = encode(img_path)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the bottleneck validation images features\n",
    "path_for_validation = os.path.join(validation_dir, 'Pickle')\n",
    "os.mkdir(path_for_validation)\n",
    "path_for_validation = os.path.join(path_for_validation, 'validation_imgs_encoded.pkl')\n",
    "with open(path_for_validation,'wb') as encoded_pickle:\n",
    "    pickle.dump(validation_imgs_encoded, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_for_validation = os.path.join(validation_dir, 'Pickle')\n",
    "train_img_features = pickle.load(open(path_for_training,'rb'))\n",
    "validation_img_features = pickle.load(open(path_for_validation,'rb'))\n",
    "print('Train img features = %d' % len(train_img_features))\n",
    "print('Validation img features = %d' % len(validation_img_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words : 12604 & number of vocabs interested : 3093\n"
     ]
    }
   ],
   "source": [
    "# Create a training vocabulary with only words that occurs at least 10 times\n",
    "word_freq = dict()\n",
    "for list_of_cap in training_captions.values():\n",
    "    for caption in list_of_cap:\n",
    "        tokens = caption.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_freq:\n",
    "                word_freq[token] = 1\n",
    "            else :\n",
    "                count = word_freq[token] + 1\n",
    "                word_freq[token] = count\n",
    "training_vocabulary = [word for word in word_freq if word_freq[word] >= 10]\n",
    "print('total number of words : %d & number of vocabs interested : %d' % (len(word_freq), len(training_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two dictionaries for easy conversion from index to word and from word to index\n",
    "index_to_word = dict()\n",
    "word_to_index = dict()\n",
    "index = 1\n",
    "for word in training_vocabulary:\n",
    "    index_to_word[index] = word\n",
    "    word_to_index[word] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(training_vocabulary) == len(index_to_word))\n",
    "print(len(training_vocabulary) == len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of all captions : 80\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length of all training captions\n",
    "max_len_of_all_cap = 0\n",
    "for captions in training_captions.values():\n",
    "    for i in range(len(captions)):\n",
    "        max_len_of_all_cap = max(len(captions[i].split()), max_len_of_all_cap)\n",
    "print(\"Max length of all captions : %d\" % max_len_of_all_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def data_generator(train_captions, train_features, wordtoix, max_length, vocab_size, num_imgs_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    count = 0\n",
    "    # loop for ever over images\n",
    "    while True:\n",
    "        for img, captions in train_captions.items():\n",
    "            count += 1\n",
    "            feature = train_features[img]\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in caption.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if count == num_imgs_per_batch:\n",
    "                yield [[np.asarray(X1), np.asarray(X2)], np.asarray(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload glove txt and create dictionary \"word_to_embedding_vectors\" where key is word and value is embedding_vector\n",
    "import io\n",
    "glove_dir = './data_folder/glove'\n",
    "\n",
    "word_to_embedding_vectors = dict()\n",
    "\n",
    "glove_file = io.open(os.path.join(glove_dir, 'glove.6B.200d.txt'), mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    tokens = line.split()\n",
    "    word = tokens[0]\n",
    "    embedding_vector = np.asarray(tokens[1:], dtype='float32')\n",
    "    word_to_embedding_vectors[word] = embedding_vector\n",
    "    \n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\n",
    "Higher-dimensional embeddings can more accurately represent the relationships between input values.\n",
    "But more dimensions increases the chance of overfitting and leads to slower training.\n",
    "Empirical rule of thumb (a good starting point but should be tuned using the validation data) : embedding_dimensions =  number_of_categories**0.25\n",
    "\n",
    "But since dimension of embedding_vectors above is 200, I'll stick to 200 for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of embedding_vector: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"dimension of embedding_vector: \" + str(len(word_to_embedding_vectors['a'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding_matrix which has key : \n",
    "embedding_dim = len(word_to_embedding_vectors.values()[0])\n",
    "vocabulary_size = len(training_vocabulary) + 1\n",
    "embedding_matrix = np.zeros((vocabulary_size,embedding_dim))\n",
    "\n",
    "# For words in our training vocabulary, extract embedding_vectors from glove if exist\n",
    "for word, index in word_to_index.items():\n",
    "    if word in word_to_embedding_vectors:\n",
    "        embedding_matrix[index] = word_to_embedding_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since order of words in captions are not important interpreting their meanings, I'm going to use conv1D instead of RNN/LSTM/GRU. If order was important usually as in problems involving time-series data, would have used RNN/LSTM. But this case, where involving text data, conv1d can be used for their lightness with almost the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model\n",
    "from keras import Input, layers, Model\n",
    "\n",
    "input1 = Input(shape=(2048,))\n",
    "x1 = layers.Dense(256, activation='relu')(input1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "input2 = Input(shape=(max_len_of_all_cap,))\n",
    "x2 = layers.Embedding(vocabulary_size, embedding_dim)(input2)\n",
    "x2 = layers.Conv1D(128, 7, activation='relu')(x2)\n",
    "x2 = layers.MaxPooling1D(5)(x2)\n",
    "x2 = layers.Conv1D(256, 7, activation='relu')(x2)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "input_added = layers.add([x1, x2])\n",
    "x3 = layers.Dense(256, activation='relu')(input_added)\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(x3)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze embedding layer\n",
    "model.layers[1].set_weights([embedding_matrix])\n",
    "model.layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_encoded = pickle.load(open('./data_folder/train/Pickle/train_imgs_encoded.pkl','rb'))\n",
    "validation_imgs_encoded = pickle.load(open('./data_folder/validation/Pickle/validation_imgs_encoded.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('./models/model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some changes\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_1' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('./models/model_19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some changes\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_2' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "input1 = Input(shape=(2048,))\n",
    "x1 = layers.Dropout(0.5)(input1)\n",
    "x1 = layers.Dense(512, activation='relu')(x1)\n",
    "\n",
    "input2 = Input(shape=(max_len_of_all_cap,))\n",
    "x2 = layers.Embedding(vocabulary_size, embedding_dim, mask_zero=True)(input2)\n",
    "x2 = layers.Dropout(0.5)(x2)\n",
    "x2 = layers.LSTM(512)(x2)\n",
    "\n",
    "input_added = layers.add([x1, x2])\n",
    "x3 = layers.Dense(512, activation='relu')(input_added)\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(x3)\n",
    "model = Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_62 (InputLayer)           (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_61 (InputLayer)           (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 80, 200)      618800      input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 2048)         0           input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 80, 200)      0           embedding_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 512)          1049088     dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 512)          1460224     dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 512)          0           dense_62[0][0]                   \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 512)          262656      add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 3094)         1587222     dense_63[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,977,990\n",
      "Trainable params: 4,977,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x170d356d0>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   8/4000 [..............................] - ETA: 2:47:26 - loss: 6.9724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-987dd59d61b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                batch_size)\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/lstm_model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/lstm_model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./models/lstm/lstm_model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images_path = './data_folder/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_folder/validation/Pickle/validation_imgs_encoded.pkl', 'rb') as encoded_pickle:\n",
    "    validation_imgs_encoded = pickle.load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "def greedySearch(feature):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_len_of_all_cap):\n",
    "        inputs = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        #\n",
    "        inputs = pad_sequences([inputs], maxlen=max_len_of_all_cap)\n",
    "\n",
    "        y_hat = model.predict([feature, inputs], verbose=0)\n",
    "        y_hat = np.argmax(y_hat)\n",
    "        word = index_to_word[y_hat]\n",
    "        \n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    predicted_caption = in_text.split()\n",
    "    # remove 'startseq' & 'endseq'\n",
    "    predicted_caption = predicted_caption[1:-1]\n",
    "    predicted_caption = ' '.join(predicted_caption)\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "from random import randint\n",
    "\n",
    "index = randint(0,1999)\n",
    "img_name = list(validation_imgs_encoded.keys())[index]\n",
    "feature = validation_imgs_encoded[img_name].reshape((1,2048))\n",
    "x = plt.imread(validation_images_path + img_name + '.jpg')\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "print(\"Greedy:\",greedySearch(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
