{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Personal Project: Guide Bot\n",
    "\n",
    "Goal of this project: To build an image-captioning application \"Guide Bot\" that can be connected to a camera and describe the scene in human voice using text-to-speech conversion. Possibly serve as an aid for the blind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< About the original dataset \"Flickr 30k Data\" >\n",
    "\n",
    "One folder of 30k images and a csv file of corresponding captions of the images (5 captions per image)\n",
    "\n",
    "Acknowledgement -- this dataset is taken from University of Illinois at Urbana-Champaign Department of Computer Science (https://forms.illinois.edu/sec/229675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Table of Contents >\n",
    "\n",
    "1) Create directories to store train / validation / test data\n",
    "\n",
    "2) From 30k images, randomly extract 12k for train set, 2k for validation set, and 2k for test set\n",
    "    - not all of 30k because only going to use CPU for training\n",
    "\n",
    "3) Extract 80k captions from csv file which contains all captions of images (5 captions per image) and save it as txt file\n",
    "    - 5 captions * 16k images (12k train + 2k validation + 2k test)\n",
    "\n",
    "4) Create a dictionary \"imgs_and_captions\" where\n",
    "    - key : name_of_images\n",
    "    - value : a list of corresponding_captions\n",
    "    \n",
    "5) Clean each caption in the dictionary \"imgs_and_captions\"\n",
    "    - remove punctuations\n",
    "    - remove non-alphabets\n",
    "    - remove trailing whitespaces\n",
    "    - convert all characters to lower-case\n",
    "    - did not remove english stopwords\n",
    "    \n",
    "6) Save cleaned captions to \"cleaned_captions.txt\"\n",
    "\n",
    "7) Create a dictionary \"training_captions\" where\n",
    "    - key : name of image (without .jpg extension)\n",
    "    - value : a list of corresponding 5 captions\n",
    "\n",
    "8) Load Inception V3 model and remove its last layer\n",
    "    - remove the last layer because the model is not used to classify images but to convert images to fixed-length informative vectors\n",
    "    \n",
    "9) Automated Feature Engineering: convert images to (2048,) sized informative vectors and create a dictionary \"train_imgs_encoded\" where\n",
    "    - key : name of image (train set)\n",
    "    - value : feature vector of size (2048,)\n",
    "    - saved as : \"train_imgs_encoded.pkl\"\n",
    "\n",
    "10) Automated Feature Engineering: convert images to (2048,) sized informative vectors and create a dictionary \"validation_imgs_encoded\" where\n",
    "    - key : name of image (validation set)\n",
    "    - value : feature vector of size (2048,)\n",
    "    - saved as : \"validation_imgs_encoded.pkl\"\n",
    "    \n",
    "11) Create a list \"training_vocabulary\" with only words that occurs at least 10 times\n",
    "\n",
    "12) Create two dictionaries for easy conversion from index to word and from word to index\n",
    "    - dictionary \"word_to_index\"\n",
    "    - dictionary \"index_to_word\"\n",
    "    \n",
    "13) Calculate max length (max number of words) of training set captions : \"max_len_of_all_cap\"\n",
    "    - to make sure each sequence is of equal length when batch processing\n",
    "\n",
    "14) Create a data_generator\n",
    "\n",
    "15) Read glove.txt and create a dictionary \"word_to_embedding_vectors\" where\n",
    "    - key : word in glove.txt\n",
    "    - value : corresponding embedding vector from pretrained Glove vectors\n",
    "    \n",
    "16) Create a dictionary \"embedding_matrix\" where\n",
    "    - key : word in training vocabulary\n",
    "    - value : embedding vector from Glove if the word exists\n",
    "\n",
    "17) Build a functional model with Dense layers and Conv1D layers\n",
    "    - Freeze embedding layer\n",
    "\n",
    "18) Build a functional model with Dense layers and LSTM layer\n",
    "    - Freeze embedding layer\n",
    "\n",
    "19) Train both models\n",
    "    - Trained only using CPU: pain in my ass\n",
    "    \n",
    "20) Using greedy search, predict captions of images in validation set\n",
    "\n",
    "21) Conclusion\n",
    "\n",
    "22) Acknowledgement for Pretrained Model Used\n",
    "\n",
    "23) Sources of Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create directories to store train / validation / test data\n",
    "# Make necessary folders for the data\n",
    "import os\n",
    "\n",
    "data_dir = './data_folder'\n",
    "os.mkdir(data_dir)\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(data_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) From 30k images, randomly extract 12k for train set, 2k for validation set, and 2k for test set\n",
    "import shutil\n",
    "original_data_dir = './flickr30k_images/flickr30k_images'\n",
    "count = 0\n",
    "all_imgs = set()\n",
    "train_imgs = set()\n",
    "val_imgs = set()\n",
    "test_imgs = set()\n",
    "for img in os.listdir(original_data_dir):\n",
    "    if (count == 16000):\n",
    "        break\n",
    "    if (count < 12000):\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(train_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        train_imgs.add(img)\n",
    "        count += 1\n",
    "    elif (count < 14000):\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(validation_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        val_imgs.add(img)\n",
    "        count += 1\n",
    "    else :\n",
    "        src = os.path.join(original_data_dir, img)\n",
    "        dst = os.path.join(test_dir, img)\n",
    "        shutil.copyfile(src, dst)\n",
    "        all_imgs.add(img)\n",
    "        test_imgs.add(img)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Trainset images : ', len(os.listdir(train_dir)))\n",
    "print('Validationset images : ', len(os.listdir(validation_dir)))\n",
    "print('Testset images : ', len(os.listdir(test_dir)))\n",
    "# \".DS_Store\" was also counted in all of the folders (therefore got 12000/2000/2000 images)\n",
    "print(len(all_imgs))\n",
    "print(len(train_imgs))\n",
    "print(len(val_imgs))\n",
    "print(len(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Extract 80k captions from csv file which contains all captions of images (5 captions per image)\n",
    "#    and save it as txt file\n",
    "\n",
    "csv_path = './flickr30k_images/results.csv'\n",
    "with open(csv_path, \"r\") as csv_file:\n",
    "    lines = [line.split(\"|\") for line in csv_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Extract 80k captions from csv file which contains all captions of images (5 captions per image)\n",
    "#    and save it as txt file\n",
    "\n",
    "txt_file = open('./data_folder/all_captions.txt',\"w\")\n",
    "\n",
    "for line in lines:\n",
    "    img_name = line[0].strip()\n",
    "    if img_name in all_imgs:\n",
    "        txt_file.write(img_name + \"#\" + str(line[1]).strip() + \" \" + line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for line in open('./data_folder/all_captions.txt',\"r\"):\n",
    "    count += 1\n",
    "print(count)\n",
    "# 286 captions are missing, but will ignore them and proceed to next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Create a dictionary \"imgs_and_captions\" where key : name_of_images\n",
    "#    and value : a list of corresponding_captions\n",
    "\n",
    "all_captions = open('./data_folder/all_captions.txt',\"r\").read()\n",
    "imgs_and_captions = dict()\n",
    "for line in all_captions.split('\\n'):\n",
    "    l = line.split(' ')\n",
    "    img_name = l[0].split('.')[0]\n",
    "    corresponding_caption = ' '.join(l[1:])\n",
    "    if img_name not in imgs_and_captions:\n",
    "        imgs_and_captions[img_name] = list()\n",
    "    imgs_and_captions[img_name].append(corresponding_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(imgs_and_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions.keys()[12345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions['2814037463'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2814037463'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions.keys()[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions['2860314714'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2860314714'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Clean each caption in the dictionary \"imgs_and_captions\"\n",
    "#    - remove punctuations\n",
    "#    - remove non-alphabets\n",
    "#    - remove trailing whitespaces\n",
    "#    - convert all characters to lower-case\n",
    "#    - did not remove english stopwords\n",
    "\n",
    "import string\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "for key, captions in imgs_and_captions.items():\n",
    "    for i in range(len(captions)):\n",
    "        tokens = captions[i].split() #split into words\n",
    "        tokens = [word.translate(None, string.punctuation) for word in tokens] #remove punctuations\n",
    "        tokens = [word for word in tokens if word.isalpha()] #remove non-alphabetics\n",
    "        tokens = [word.strip() for word in tokens] #remove trailing whitespaces\n",
    "        tokens = [word.lower() for word in tokens] #convert to lower-case \n",
    "#         tokens = [word for word in tokens if word not in set (stopwords.words('english'))] #remove stopwords        \n",
    "        captions[i] = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions['2814037463'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2814037463'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions['2860314714'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2860314714'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs_and_captions['2514612680'])\n",
    "print(\"number of captions: \" + str(len(imgs_and_captions['2514612680'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Save cleaned captions to \"cleaned_captions.txt\"\n",
    "\n",
    "txt_file = open('./data_folder/cleaned_captions.txt',\"w\")\n",
    "for img, captions in imgs_and_captions.items():\n",
    "    for caption in captions:\n",
    "        txt_file.write(img + \" \" + caption + '\\n')\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_imgs))\n",
    "print(len(train_imgs))\n",
    "print(len(val_imgs))\n",
    "print(len(test_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of names of imgs in the trainset (without .jpg)\n",
    "train_img_names = []\n",
    "for img in train_imgs:\n",
    "    train_img_names.append(img.split('.')[0])\n",
    "print(len(train_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Create a dictionary \"training_captions\" where key : name of image (without .jpg extension)\n",
    "#    and value : a list of corresponding 5 captions\n",
    "\n",
    "cleaned_captions = open('./data_folder/cleaned_captions.txt',\"r\").read()\n",
    "training_captions = dict()\n",
    "for line in cleaned_captions.split('\\n'):\n",
    "    tokens = line.split()\n",
    "    if (len(tokens) < 2):\n",
    "        continue\n",
    "    img, caption = tokens[0], tokens[1:]\n",
    "    if img in train_img_names:\n",
    "        if img not in training_captions:\n",
    "            training_captions[img] = list()\n",
    "        words = 'startseq ' + ' '.join(caption) + ' endseq'\n",
    "        training_captions[img].append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_captions))\n",
    "print(training_captions.keys()[36])\n",
    "print(training_captions['3178005751'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_captions.keys()[6789])\n",
    "print(training_captions['12974441'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Load Inception V3 model and remove its last layer\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Load Inception V3 model and remove its last layer\n",
    "\n",
    "from keras.models import Model\n",
    "model_v3_without_output_layer = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(image_path):\n",
    "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Function to encode a given image into a vector of size (2048, )\n",
    "def encode(image):\n",
    "    image = preprocess(image) # preprocess the image\n",
    "    fea_vec = model_v3_without_output_layer.predict(image) # Get the encoding vector for the image\n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Automated Feature Engineering: convert images to (2048,) sized informative vectors\n",
    "#    and create a dictionary \"train_imgs_encoded\" where key : name of image (train set)\n",
    "#    and value : feature vector of size (2048,), saved as : \"train_imgs_encoded.pkl\"\n",
    "\n",
    "train_imgs_encoded = dict()\n",
    "for img in os.listdir(train_dir):\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(train_dir, img)\n",
    "    train_imgs_encoded[img_name] = encode(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Automated Feature Engineering: convert images to (2048,) sized informative vectors\n",
    "#    and create a dictionary \"train_imgs_encoded\" where key : name of image (train set)\n",
    "#    and value : feature vector of size (2048,), saved as : \"train_imgs_encoded.pkl\"\n",
    "\n",
    "import pickle\n",
    "path_for_training = os.path.join(train_dir, 'Pickle')\n",
    "os.mkdir(path_for_training)\n",
    "path_for_training = os.path.join(path_for_training, 'train_imgs_encoded.pkl')\n",
    "with open(path_for_training,'wb') as encoded_pickle:\n",
    "    pickle.dump(train_imgs_encoded, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10) Automated Feature Engineering: convert images to (2048,) sized informative vectors\n",
    "#     and create a dictionary \"validation_imgs_encoded\" where key : name of image (validation set)\n",
    "#     and value : feature vector of size (2048,), saved as : \"validation_imgs_encoded.pkl\"\n",
    "\n",
    "validation_imgs_encoded = dict()\n",
    "count = 1\n",
    "for img in os.listdir(validation_dir):\n",
    "    if img == '.DS_Store':\n",
    "        continue\n",
    "    img_name = img.split('.')[0]\n",
    "    img_path = os.path.join(validation_dir, img)\n",
    "    print(\"\" + str(count) + \" / 2000 encoding...\")\n",
    "    validation_imgs_encoded[img_name] = encode(img_path)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Automated Feature Engineering: convert images to (2048,) sized informative vectors\n",
    "#     and create a dictionary \"validation_imgs_encoded\" where key : name of image (validation set)\n",
    "#     and value : feature vector of size (2048,), saved as : \"validation_imgs_encoded.pkl\"\n",
    "\n",
    "path_for_validation = os.path.join(validation_dir, 'Pickle')\n",
    "os.mkdir(path_for_validation)\n",
    "path_for_validation = os.path.join(path_for_validation, 'validation_imgs_encoded.pkl')\n",
    "with open(path_for_validation,'wb') as encoded_pickle:\n",
    "    pickle.dump(validation_imgs_encoded, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_for_validation = os.path.join(validation_dir, 'Pickle')\n",
    "train_img_features = pickle.load(open(path_for_training,'rb'))\n",
    "validation_img_features = pickle.load(open(path_for_validation,'rb'))\n",
    "print('Train img features = %d' % len(train_img_features))\n",
    "print('Validation img features = %d' % len(validation_img_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Create a list \"training_vocabulary\" with only words that occurs at least 10 times\n",
    "\n",
    "word_freq = dict()\n",
    "for list_of_cap in training_captions.values():\n",
    "    for caption in list_of_cap:\n",
    "        tokens = caption.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_freq:\n",
    "                word_freq[token] = 1\n",
    "            else :\n",
    "                count = word_freq[token] + 1\n",
    "                word_freq[token] = count\n",
    "training_vocabulary = [word for word in word_freq if word_freq[word] >= 10]\n",
    "print('total number of words : %d & number of vocabs interested : %d' % (len(word_freq), len(training_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Create two dictionaries for easy conversion from index to word and from word to index\n",
    "#     dictionary \"word_to_index\"\n",
    "#     dictionary \"index_to_word\"\n",
    "\n",
    "index_to_word = dict()\n",
    "word_to_index = dict()\n",
    "index = 1\n",
    "for word in training_vocabulary:\n",
    "    index_to_word[index] = word\n",
    "    word_to_index[word] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary word_to_index as text file\n",
    "# index starts from 1\n",
    "txt_file = open('./data_folder/word_to_index.txt',\"w\")\n",
    "for word, index in word_to_index.items():\n",
    "    txt_file.write(word + \" \" + str(index) + '\\n')\n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_vocabulary) == len(index_to_word))\n",
    "print(len(training_vocabulary) == len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Calculate max length (max number of words) of training set captions : \"max_len_of_all_cap\"\n",
    "max_len_of_all_cap = 0\n",
    "for captions in training_captions.values():\n",
    "    for i in range(len(captions)):\n",
    "        max_len_of_all_cap = max(len(captions[i].split()), max_len_of_all_cap)\n",
    "print(\"Max length of all captions : %d\" % max_len_of_all_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Create a data_generator\n",
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def data_generator(train_captions, train_features, wordtoix, max_length, vocab_size, num_imgs_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    count = 0\n",
    "    # loop for ever over images\n",
    "    while True:\n",
    "        for img, captions in train_captions.items():\n",
    "            count += 1\n",
    "            feature = train_features[img]\n",
    "            for caption in captions:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in caption.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if count == num_imgs_per_batch:\n",
    "                yield [[np.asarray(X1), np.asarray(X2)], np.asarray(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Read glove.txt and create a dictionary \"word_to_embedding_vectors\" where key : word in glove.txt\n",
    "#     and value : corresponding embedding vector from pretrained Glove vectors\n",
    "\n",
    "import io\n",
    "glove_dir = './data_folder/glove'\n",
    "\n",
    "word_to_embedding_vectors = dict()\n",
    "\n",
    "glove_file = io.open(os.path.join(glove_dir, 'glove.6B.200d.txt'), mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    tokens = line.split()\n",
    "    word = tokens[0]\n",
    "    embedding_vector = np.asarray(tokens[1:], dtype='float32')\n",
    "    word_to_embedding_vectors[word] = embedding_vector\n",
    "    \n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\n",
    "\n",
    "Higher-dimensional embeddings can more accurately represent the relationships between input values.\n",
    "But more dimensions increases the chance of overfitting and leads to slower training.\n",
    "Empirical rule of thumb (a good starting point but should be tuned using the validation data) : embedding_dimensions =  number_of_categories**0.25\n",
    "\n",
    "But since dimension of embedding_vectors above is 200, will stick to 200 for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dimension of embedding_vector: \" + str(len(word_to_embedding_vectors['a'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Create a dictionary \"embedding_matrix\" where key : word in training vocabulary\n",
    "#     and value : embedding vector from Glove if the word exists\n",
    "\n",
    "embedding_dim = len(word_to_embedding_vectors.values()[0])\n",
    "vocabulary_size = len(training_vocabulary) + 1\n",
    "embedding_matrix = np.zeros((vocabulary_size,embedding_dim))\n",
    "\n",
    "# For word in our training vocabulary, extract embedding_vector from Glove if exists\n",
    "for word, index in word_to_index.items():\n",
    "    if word in word_to_embedding_vectors:\n",
    "        embedding_matrix[index] = word_to_embedding_vectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since order of words in captions are not important interpreting their meanings, will use conv1D instead of RNN/LSTM/GRU. If order was important usually as in problems involving time-series data, would have used RNN/LSTM. But in this case, where involving text data, conv1d can be used for their lightness with almost the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Build a functional model with Dense layers and Conv1D layers\n",
    "#     - Freeze embedding layer\n",
    "\n",
    "from keras import Input, layers, Model\n",
    "\n",
    "input1 = Input(shape=(2048,))\n",
    "x1 = layers.Dense(256, activation='relu')(input1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "input2 = Input(shape=(max_len_of_all_cap,))\n",
    "x2 = layers.Embedding(vocabulary_size, embedding_dim)(input2)\n",
    "x2 = layers.Conv1D(128, 7, activation='relu')(x2)\n",
    "x2 = layers.MaxPooling1D(5)(x2)\n",
    "x2 = layers.Conv1D(256, 7, activation='relu')(x2)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "input_added = layers.add([x1, x2])\n",
    "x3 = layers.Dense(256, activation='relu')(input_added)\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(x3)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze embedding layer\n",
    "model.layers[1].set_weights([embedding_matrix])\n",
    "model.layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_encoded = pickle.load(open('./data_folder/train/Pickle/train_imgs_encoded.pkl','rb'))\n",
    "validation_imgs_encoded = pickle.load(open('./data_folder/validation/Pickle/validation_imgs_encoded.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('./models/model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some changes\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_1' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('./models/model_19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some changes\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 3\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/model_2' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Build a functional model with Dense layers and LSTM layer\n",
    "#     - Freeze embedding layer\n",
    "\n",
    "input1 = Input(shape=(2048,))\n",
    "x1 = layers.Dropout(0.5)(input1)\n",
    "x1 = layers.Dense(256, activation='relu')(x1)\n",
    "\n",
    "input2 = Input(shape=(max_len_of_all_cap,))\n",
    "x2 = layers.Embedding(vocabulary_size, embedding_dim, mask_zero=True)(input2)\n",
    "x2 = layers.Dropout(0.5)(x2)\n",
    "x2 = layers.LSTM(256)(x2)\n",
    "\n",
    "input_added = layers.add([x1, x2])\n",
    "x3 = layers.Dense(256, activation='relu')(input_added)\n",
    "output = layers.Dense(vocabulary_size, activation='softmax')(x3)\n",
    "model = Model(inputs=[input1, input2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 5\n",
    "steps = len(training_captions) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(training_captions,\n",
    "                               train_imgs_encoded,\n",
    "                               word_to_index,\n",
    "                               max_len_of_all_cap,\n",
    "                               vocabulary_size,\n",
    "                               batch_size)\n",
    "    history = model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('./models/lstm_model_1' + str(i) + '.h5')\n",
    "    # loss: 2.7302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model and embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./models/lstm/lstm_model_9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images_path = './data_folder/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_folder/validation/Pickle/validation_imgs_encoded.pkl', 'rb') as encoded_pickle:\n",
    "    validation_imgs_encoded = pickle.load(encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) Using greedy search, predict captions of images in validation set\n",
    "# Reference from: https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "\n",
    "def greedy_search(feature):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_len_of_all_cap):\n",
    "        inputs = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        inputs = pad_sequences([inputs], maxlen=max_len_of_all_cap)\n",
    "\n",
    "        y_hat = model.predict([feature, inputs], verbose=0)\n",
    "        y_hat = np.argmax(y_hat)\n",
    "        word = index_to_word[y_hat]\n",
    "        \n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    predicted_caption = in_text.split()\n",
    "    # remove 'startseq' & 'endseq'\n",
    "    predicted_caption = predicted_caption[1:-1]\n",
    "    predicted_caption = ' '.join(predicted_caption)\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "index = randint(0,1999)\n",
    "img_name = list(validation_imgs_encoded.keys())[index]\n",
    "feature = validation_imgs_encoded[img_name].reshape((1,2048))\n",
    "\n",
    "x = plt.imread(validation_images_path + img_name + '.jpg')\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "caption = greedy_search(feature)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Conclusion >\n",
    "\n",
    "The application works, but the deep learning model used seems too weak. Since only used CPU for training, I was not able to train the model for enough epochs and had to use a small amount of data. Utilizing GPU, I need to collect and use more data (images and captions) and tweak hyper-parameters of the model. Also need to find the right evaluation metrics for training.\n",
    "\n",
    "I built two different models: one using Conv1D and the other using LSTM. Since order of words in captions are not important interpreting their meanings, I thought using Conv1D instead of RNN/LSTM/GRU seemed like a more efficient approach. Conv1d is known to be lighter and can attain almost the same performance. Due to limited resources, I was not able to fully compare and contrast Conv1d and LSTM models. But each epoch in the Conv1D model was a lot shorter than that in the LSTM model, which proves the lightness of Conv1D. However, the Conv1D model also learned at a slower rate than the LSTM model, resulting in requiring more epochs for training. I wasn't able to determine which model is better from this project, and I'll have to closely examine it in my future projects.\n",
    "\n",
    "Image-captioning to help the blind seems extremely difficuly because it requires tremendous amount of data (images and captions) for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Acknowledgement for Pretrained Model Used >\n",
    "\n",
    "1) Inception V3 (from keras.applications)\n",
    "    - utilized for extracting feature vectors from images\n",
    "\n",
    "2) Glove: Global Vectors for Word Representation\n",
    "    by Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.\n",
    "    source from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Sources of Reference >\n",
    "\n",
    "1) Francois Chollet.(2017) \"Deep Learning with Python\" Published by Manning\n",
    "\n",
    "2) https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
    "\n",
    "3) https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8?fbclid=IwAR2O1DXkN305efbxVazsbV-rmLKR7fsUvq39jUa5CydHEU3xKeytCx_ycsw\n",
    "\n",
    "4) https://github.com/hlamba28/Automatic-Image-Captioning\n",
    "\n",
    "5) https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_gui/py_video_display/py_video_display.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
